### main purpose: run the MoChA pipeline on Google cloud for genotype calling (i.e., from Illumina .idat data to genotype calls)


## MoChA pipeline reference: https://github.com/freeseek/mocha/tree/master/wdl (developed by Giulio Genovese)
## All the codes below related to the Google Cloud, Cromwell, and MoChA are copied from Giulio's MoChA pipeline (link shown above)
## Only codes specific to the Pakistani samples are created by Jiayi Xu 


#Initialize your Google Cloud configuration:

gcloud init

#Download a private json key for your service account through the following command:

gcloud iam service-accounts keys create jpc-genetics.key.json --iam-account=792128648090-compute@developer.gserviceaccount.com
#created key [ec54d15b33032343d15f16010e49d985f9a89c69] of type [json] as [jpc-genetics.key.json] for [792128648090-compute@developer.gserviceaccount.com]

#Enable the Cloud Life Sciences API, Compute Engine API, and Google Cloud Storage JSON API for your Google project

for api in lifesciences compute storage-api; do
gcloud services enable $api.googleapis.com
done

#You need the following roles available to your service account: Cloud Life Sciences Workflows Runner, Service Account User, Storage Object Admin

for role in lifesciences.workflowsRunner iam.serviceAccountUser storage.objectAdmin; do
gcloud projects add-iam-policy-binding jpc-genetics --member serviceAccount:792128648090-compute@developer.gserviceaccount.com --role roles/$role
done

#Create a Google virtual machine (VM) with name INSTANCE-ID from the VM instances page (for this project: https://console.cloud.google.com/compute/instances?project=jpc-genetics). 
#The n1-standard-1 (1 vCPU, 3.75 GB memory) VM with Ubuntu 21.04 will be sufficient, 
#but make sure to provide at least 100GB of space for the boot disk, rather than the default 10GB, as the mySQL database will easily fail with the default space settings


#start the virtual machine

gcloud compute instances start jiayi-vm --project jpc-genetics --zone us-east1-b

#Copy the private json key for your service account to the VM:

gcloud compute scp jpc-genetics.key.json  jiayi-vm: --project jpc-genetics --zone us-east1-b

#Updating project ssh metadata...â §Updated [https://www.googleapis.com/compute/v1/projects/jpc-genetics].
#Updating project ssh metadata...done.
#Waiting for SSH key to propagate.
#Warning: Permanently added 'compute.2630457628485642250' (ECDSA) to the list of known hosts.
#jpc-genetics.key.json  


#Login to the VM with the following command (notice this command will enable ssh tunneling):

gcloud compute ssh jiayi-vm --project jpc-genetics --zone us-east1-b -- -L 8000:localhost:8000

#As the suggested VM costs approximately $35/month to run, when you are not running any computations, to avoid incurring unnecessary costs, 
#you can stop the virtual machine with the following command:

gcloud compute instances stop jiayi-ID --project jpc-genetics --zone us-east1-b


## the storage place in Google bucket for the Pakistani GSA data
gsutil ls -l gs://bigdeli-working/pakistan/


#in VM

#install some basic packages as well as the WOMtool and the Cromwell server:

sudo apt update && sudo apt install default-jre-headless mysql-server jq
wget https://github.com/broadinstitute/cromwell/releases/download/58/womtool-58.jar
wget https://github.com/broadinstitute/cromwell/releases/download/58/cromwell-58.jar

#Download the PAPIv2.conf configuration file with the following command:

wget https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/PAPIv2.conf

#Add the webservice stanza to the PAPIv2.conf configuration file to make sure the server will only be accessible from the local machine 
#(by default it is open to any interface):

## the content in the PAPIv2.conf after following the instruction in the MoChA WDL pipeline (https://github.com/freeseek/mocha/tree/master/wdl)


# This is an example of how you can use the Google Pipelines API backend
# provider. *This is not a complete configuration file!* The
# content here should be copy pasted into the backend -> providers section
# of the cromwell.examples.conf in the root of the repository. You should
# uncomment lines that you want to define, and read carefully to customize
# the file. If you have any questions, please open an issue at
# https://broadworkbench.atlassian.net/projects/BA/issues
# Documentation
# https://cromwell.readthedocs.io/en/stable/backends/Google/
       
webservice {
           port = 8000
           interface = 127.0.0.1
}
google {
           application-name = "cromwell"
           auths = [
            {
             name = "service-account"
             scheme = "service_account"
             json-file = "jpc-genetics.key.json"
            }
           ]
}
engine {
          filesystems {
             gcs {
                 auth = "service-account"
                 project = "jpc-genetics"
             }
          }
}
database {
           profile = "slick.jdbc.MySQLProfile$"
           db {
              driver = "com.mysql.cj.jdbc.Driver"
              url = "jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true"
              user = "user"
              password = "pass"
              connectionTimeout = 5000
           }
}
call-caching {
  enabled = true
  invalidate-bad-cache-results = true
}
        

backend {
  default = PAPIv2
  providers {
    PAPIv2 {
      actor-factory = "cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory"
      config {
        # Google project
        project = "jpc-genetics"
    
        # Base bucket for workflow executions
        root = "gs://bigdeli-working/pakistan/cromwell/cromwell-executions"
    
        # Make the name of the backend used for call caching purposes insensitive to the PAPI version.
        name-for-call-caching-purposes: PAPI
        # Emit a warning if jobs last longer than this amount of time. This might indicate that something got stuck in PAPI.
        slow-job-warning-time: 24 hours
        # Whether to allow users to specify the 'noAddress' runtime attribute when submitting jobs.
        # Defaults to true.
        # If set to false, the noAddress attribute will be ignored and public addresses will always be assigned.
        allow-noAddress-attribute: true
        # Set this to the lower of the two values "Queries per 100 seconds" and "Queries per 100 seconds per user" for
        # your project.
        #
        # Used to help determine maximum throughput to the Google Genomics API. Setting this value too low will
        # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.
        # 1000 is the default "Queries per 100 seconds per user", 50000 is the default "Queries per 100 seconds"
        # See https://cloud.google.com/genomics/quotas for more information
        genomics-api-queries-per-100-seconds = 1000
    
        # Polling for completion backs-off gradually for slower-running jobs.
        # This is the maximum polling interval (in seconds):
        maximum-polling-interval = 600
        # Optional Dockerhub Credentials. Can be used to access private docker images.
        dockerhub {
          # account = ""
          # token = ""
        }
        # Number of workers to assign to PAPI requests
        request-workers = 3
        # Optional configuration to use high security network (Virtual Private Cloud) for running jobs.
        # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.
        # virtual-private-cloud {
        #  network-label-key = "network-key"
        #  auth = "application-default"
        # }
        # Global pipeline timeout
        # Defaults to 7 days; max 30 days
        # pipeline-timeout = 7 days

        genomics {
          # A reference to an auth defined in the `google` stanza at the top.  This auth is used to create
          # Pipelines and manipulate auth JSONs.
          auth = "service-account"
    
    
          // alternative service account to use on the launched compute instance
          // NOTE: If combined with service account authorization, both that serivce account and this service account
          // must be able to read and write to the 'root' GCS path
          compute-service-account = "default"
    
          # Endpoint for APIs, no reason to change this unless directed by Google.
          endpoint-url = "https://lifesciences.googleapis.com/"
          # Currently Cloud Life Sciences API is available only in `us-central1` and `europe-west2` locations.
          location = "us-central1"
    
          # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service
          # account not owned by the submitting user
          restrict-metadata-access = false
          
          # Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted
          # There is no logic to determine if the error was transient or not, everything is retried upon failure
          # Defaults to 3
          localization-attempts = 3
          # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.
          # Parallel composite uploads can result in a significant improvement in delocalization speed for large files
          # but may introduce complexities in downloading such files from GCS, please see
          # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.
          #
          # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off
          # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.
          parallel-composite-upload-threshold="150M"
        }
        # Controls how batched requests to PAPI are handled:
        batch-requests {
          timeouts {
            # Timeout when attempting to connect to PAPI to make requests:
            # read = 10 seconds
            # Timeout waiting for batch responses from PAPI:
            #
            # Note: Try raising this value if you see errors in logs like:
            #   WARN  - PAPI request worker PAPIQueryWorker-[...] terminated. 99 run creation requests, 0 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch requ
est failed, they might be run twice.
            #   ERROR - Read timed out
            # connect = 10 seconds
          }
        }
        
        filesystems {
          gcs {
            # A reference to a potentially different auth for manipulating files via engine functions.
            auth = "service-account"
            # Google project which will be billed for the requests
            project = "jpc-genetics"
    
            caching {
              # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs
              # Possible values: "copy", "reference". Defaults to "copy"
              # "copy": Copy the output files
              # "reference": DO NOT copy the output files but point to the original output files instead.
              #              Will still make sure than all the original output files exist and are accessible before
              #              going forward with the cache hit.
              duplication-strategy = "copy"
            }
          }
        }
    
        default-runtime-attributes {
          cpu: 1
          failOnStderr: false
          continueOnReturnCode: 0
          memory: "2048 MB"
          bootDiskSizeGb: 10
          # Allowed to be a String, or a list of Strings
          disks: "local-disk 10 SSD"
          noAddress: false
          preemptible: 0
          zones: ["us-central1-a", "us-central1-b"]
        }
        include "papi_v2_reference_image_manifest.conf"
      }
    }
  }
}

# the end of the content in the PAPIv2.conf 


#Start the mySQL server and initialize the root user with the following command (use cromwell as the default root password):
sudo systemctl start mysql
sudo mysql_secure_installation


#Login into the mySQL database and run the following commands to create a database to be used by Cromwell:
sudo mysql --user=root --password=cromwell --execute "SET GLOBAL validate_password.policy=LOW"
sudo mysql --user=root --password=cromwell --execute "SET GLOBAL validate_password.length=4"
sudo mysql --user=root --password=cromwell --execute "CREATE USER 'user'@'localhost' IDENTIFIED BY 'pass'"
sudo mysql --user=root --password=cromwell --execute "GRANT ALL PRIVILEGES ON * . * TO 'user'@'localhost'"
sudo mysql --user=root --password=cromwell --execute "CREATE DATABASE cromwell"


#download mocha pipeline
curl https://raw.githubusercontent.com/freeseek/mocha/master/wdl/mocha.wdl -o mocha.wdl


#Create an options.json file for Cromwell that should look like this:
# example:
{
  "delete_intermediate_output_files": true,
  "final_workflow_outputs_dir": "gs://{google-bucket}/cromwell/outputs",
  "use_relative_output_paths": true,
  "final_workflow_log_dir": "gs://{google-bucket}/cromwell/wf_logs",
  "final_call_logs_dir": "gs://{google-bucket}/cromwell/call_logs"
}


#create the option json using the real data
{
  "delete_intermediate_output_files": true,
  "final_workflow_outputs_dir": "gs://bigdeli-working/pakistan/cromwell/outputs",
  "use_relative_output_paths": true,
  "final_workflow_log_dir": "gs://bigdeli-working/pakistan/cromwell/wf_logs",
  "final_call_logs_dir": "gs://bigdeli-working/pakistan/cromwell/call_logs"
}


#create the {sample-set-id}.json

#example in the MoChA pipeline (https://github.com/freeseek/mocha/tree/master/wdl#running-with-cromwell-on-google-cloud-platform)
{
  "mocha.sample_set": "hapmap370k",
  "mocha.mode": "idat",
  "mocha.realign": true,
  "mocha.max_win_size_cm": 300.0,
  "mocha.overlap_size_cm": 5.0,
  "mocha.ref_name": "GRCh38",
  "mocha.ref_path": "gs://{google-bucket}/GRCh38",
  "mocha.manifest_path": "gs://{google-bucket}/manifests",
  "mocha.data_path": "gs://{google-bucket}/idats",
  "mocha.batch_tsv_file": "gs://{google-bucket}/hapmap370k.batch.tsv",
  "mocha.sample_tsv_file": "gs://{google-bucket}/hapmap370k.sample.tsv",
  "mocha.ped_file": "gs://{google-bucket}/hapmap370k.ped",
  "mocha.do_not_check_bpm": true
}

#ped_file: optional PED file for improved phasing with trios
#do_not_check_bpm: do not check whether BPM and GTC files match manifest file name. True if Illumina manifest being in an old format
#mode: idat, gtc, cel, chp, txt, vcf, or pvcf
#realign: whether manifest file should be realigned (not in vcf or pvcf mode) 
#max_win_size_cm: maximum windows size in cM for phasing
#overlap_size_cm: required overlap size in cM for consecutive windows 
#ref_name: name of reference genome, with resource default files for GRCh37 and GRCh38
#ref_path: path for reference genome resources (needed unless all resources are provided with full path)
#manifest_path: path for manifest file resources if these are provided without path 
#data_path: path for data files (overrides path column in batch_tsv_file)
#batch_tsv_file: TSV file with batch information
#sample_tsv_file: TSV file with sample information

#real data
#create the pakistan_gsa.json
{
  "mocha.sample_set_id": "pakistan_gsa",
  "mocha.mode": "idat",
  "mocha.realign": true,
  "mocha.max_win_size_cm": 300.0,
  "mocha.overlap_size_cm": 5.0,
  "mocha.ref_name": "GRCh38",
  "mocha.ref_path": "gs://bigdeli-working/pakistan/GRCh38",
  "mocha.manifest_path": "gs://bigdeli-working/pakistan/manifests",
  "mocha.data_path": "gs://bigdeli-working/pakistan/idats",
  "mocha.batch_tsv_file": "gs://bigdeli-working/pakistan/samples/pakistan_gsa.batch.tsv",
  "mocha.sample_tsv_file": "gs://bigdeli-working/pakistan/samples/pakistan_gsa.sample.tsv"
}


#Now it would be a good time to test whether the configuration worked well. Edit and generate the following hello.wdl workflow file:

version development

workflow myWorkflow {
  call myTask
}

task myTask {
  command {
    echo "hello world"
  }
  output {
    String out = read_string(stdout())
  }
  runtime {
    docker: "ubuntu:latest"
  }
}


#to start the cromwell server on google VM
(java -XX:MaxRAMPercentage=90 -Dconfig.file=PAPIv2.conf -jar cromwell-58.jar server &)

# Submit the testing workflow to the Cromwell server:
java -jar cromwell-58.jar submit hello.wdl

# the job is running successfully if you receive output like this:

[yyyy-mm-dd hh:mm:ss,ss] [info] Slf4jLogger started
[yyyy-mm-dd hh:mm:ss,ss] [info] Workflow 01234567-89ab-dcef-0123-456789abcdef submitted to http://localhost:8000


#If you get the error Bind failed for TCP channel on endpoint [/127.0.0.1:8000], it means some other service is already using port 8000.
# Run lsof -i:8000 to see which service. If it is another Cromwell server, you can stop that with:

killall java

#To verify that your input {sample-set-id}.json file is correcly formatted, you can use the WOMtool as follows:
java -jar womtool-58.jar validate mocha.wdl -i {sample-set-id}.json

#example of output from the command above
#Required workflow input 'mocha.mode' not specified
#Required workflow input 'mocha.batch_tsv_file' not specified
#Required workflow input 'mocha.sample_tsv_file' not specified
#Required workflow input 'mocha.sample_set_id' not specified


#to submit a job for the real data
java -jar cromwell-58.jar submit mocha.wdl -i pakistan_gsa.json -o options.json

#message received: workflow 170cfe7f-d3cb-4933-bb2f-206fc8e309c0 submitted

#To monitor the status of jobs submitted to the server, you can use:
curl -X GET http://localhost:8000/api/workflows/v1/query | jq

#To monitor the status of a specific job and extract the metadata:
curl -X GET http://localhost:8000/api/workflows/v1/{id}/metadata | jq
#id is generated when submit the wdl
#id example: 0a43e93f-f1ab-4987-a514-23ceeadf02a2

#example:
curl -X GET http://localhost:8000/api/workflows/v1/170cfe7f-d3cb-4933-bb2f-206fc8e309c0/metadata | jq

# how to save the workflow status in a .json file
curl -X GET http://localhost:8000/api/workflows/v1/387e0eda-0a46-4921-a6ac-ef12dd5dbe9f/metadata > 387e0eda-0a46-4921-a6ac-ef12dd5dbe9f.metadata.json

#To monitor a submitted job with workflow ID {id} just open your browser and go to URL:
http://localhost:8000/api/workflows/v1/{id}/timing

#To abort a running job, you can run:
curl -X POST http://localhost:8000/api/workflows/v1/{id}/abort | jq


#If you want to clean temporary files and log files, after you have properly moved all the output files you need, 
#you can delete the workflow executions directory defined as root in the PAPIv2.conf configuration file.

gsutil -m rm -r gs://bigdeli-working/pakistan/cromwell/cromwell-executions


#Notice that either flushing the metadata from the database or removing the workflow executions files will invalidate the cache from previously run tasks


# how to save the workflow status .json file in the google bucket
#current workflow number: workflow 387e0eda-0a46-4921-a6ac-ef12dd5dbe9f submitted (for batch_id=1)
gsutil cp 387e0eda-0a46-4921-a6ac-ef12dd5dbe9f.metadata.json gs://bigdeli-working/pakistan/testing

#troubleshooting
#gsutil ls -lR gs://bigdeli-working/pakistan/cromwell/cromwell-executions | grep idx2gtc2vcf_files


########################################################################################################
############# The script above installs the cromwell server and run the MoChA pipeline #################
########################################################################################################


###############################################################################################################################################
############# The script below is to organize the real data in the Google bucket so that it can be read by the MoChA pipeline #################
######################################################################################################################################

#For the Pakistani project

#copy files from Google bucket to VM
#gcloud compute scp gs://bigdeli-working/pakistan/ jiayi-vm

#access to Illumina Array Analysis Platform (IAAP)
#Notice that if you want to use the idat mode, this requires to run either the Illumina Array Analysis Platform Genotyping Command Line Interface 
#or the Illumina AutoConvert Software. Neither software is free (see EULA) and both are covered by a patent expiring in 2024 
#so check first to make sure you are allowed to run this software.

#It turns out that I don't need to install anything, the IAAP can be accessed by the MoChA pipeline automatically 

#install IAAP
#mkdir -p $HOME/bin && cd /tmp
#wget ftp://webdata2:webdata2@ussd-ftp.illumina.com/downloads/software/iaap/iaap-cli-linux-x64-1.1.0.tar.gz
#tar xzvf iaap-cli-linux-x64-1.1.0.tar.gz -C $HOME/bin/ iaap-cli-linux-x64-1.1.0/iaap-cli --strip-components=1
#mkdir -p $HOME/bin && cd /tmp
#gsutil cp gs://bigdeli-working/pakistan/testing/AutoConvertInstaller.msi .
#msiextract AutoConvertInstaller.msi
#cp -R Illumina/AutoConvert\ 2.0 $HOME/bin/autoconvert
#wget ftp://webdata2:webdata2@ussd-ftp.illumina.com/downloads/software/genomestudio/genomestudio-software-v2-0-4-5-installer.zip
#unzip -oj genomestudio-software-v2-0-4-5-installer.zip
#cabextract GenomeStudioInstaller.exe
#msiextract a0
#cp Illumina/GenomeStudio\ 2.0/Heatmap.dll $HOME/bin/autoconvert/
#wget https://raw.githubusercontent.com/freeseek/gtc2vcf/master/nearest_neighbor.c
#gcc -fPIC -shared -O2 -o $HOME/bin/autoconvert/libMathRoutines.dll.so nearest_neighbor.c

#check failed samples and duplicates
#The pipeline will perform a minimal amount of quality control. It is up to the user to correctly handle failed samples and duplicates.
#will check for duplicates at the genotype QC step

#Manifest files
#For Illumina data you will need to provide a CSV manifest file, a BPM manifest file, and an EGT cluster file. 
#Illumina provides these files for their arrays here or through their customer support. 

#We recommend to always use GRCh38


#Manifest files for GSA v3.0 (GRCh38) are downloaded
#CSV
wget https://support.illumina.com/content/dam/illumina-support/documents/downloads/productfiles/global-screening-array-24/v3-0/GSA-24v3-0-A2-manifest-file-csv.zip
#BPM
wget https://support.illumina.com/content/dam/illumina-support/documents/downloads/productfiles/global-screening-array-24/v3-0/GSA-24v3-0-A2-manifest-file-bpm.zip
#Cluster
wget https://support.illumina.com/content/dam/illumina-support/documents/downloads/productfiles/global-screening-array-24/v3-0/GSA-24v3-0-A1-cluster-file.zip


#download the reference genome file (GRCh38)
wget http://software.broadinstitute.org/software/mocha/mocha.GRCh38.zip

#input data
#allowed columns in the sample table: sample_id, batch_id, green_idat, red_idat for idat files
# all in one batch 
# based on Giulio's suggestion:  that is not advisable as it would create very small batches. Also, for running MoChA, 
#some adjustments would only happen when running many samples together. I usually recommend a few thousand samples per batch if possible. I would run as one batch.

#make sure the sample_id and batch_id columns have all unique IDs
#allowed columns in the batch table: batch_id, csv, bpm, egt (required) and path (optional) for idat files


#copy all the idats data into one folder
#AccessDeniedException: 403 Insufficient Permission 

#to authorize myself, type in:
gcloud auth login

#move all the idats to one folder

gsutil cp gs://bigdeli-working/pakistan/TD01510_Charney_GSA/outgoing.TD01510.Charney.PlatesP1-P4/outgoing.TD01510.PlatesP1-P2/203903540103/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01510_Charney_GSA/outgoing.TD01510.Charney.PlatesP1-P4/outgoing.TD01510.PlatesP1-P2/203903540109/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01510_Charney_GSA/outgoing.TD01510.Charney.PlatesP1-P4/outgoing.TD01510.PlatesP1-P2/203925040061/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01510_Charney_GSA/outgoing.TD01510.Charney.PlatesP1-P4/outgoing.TD01510.PlatesP1-P2/203925040068/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01510_Charney_GSA/outgoing.TD01510.Charney.PlatesP1-P4/outgoing.TD01510.PlatesP1-P2/203925040069/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01510_Charney_GSA/outgoing.TD01510.Charney.PlatesP1-P4/outgoing.TD01510.PlatesP1-P2/203925040071/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01510_Charney_GSA/outgoing.TD01510.Charney.PlatesP1-P4/outgoing.TD01510.PlatesP1-P2/203925040072/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01510_Charney_GSA/outgoing.TD01510.Charney.PlatesP1-P4/outgoing.TD01510.PlatesP1-P2/203925040086/*idat gs://bigdeli-working/pakistan/idats

gsutil cp gs://bigdeli-working/pakistan/TD01510_Charney_GSA/outgoing.TD01510.Charney.PlatesP1-P4/outgoing.TD01510.PlatesP3-P4/203865420004/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01510_Charney_GSA/outgoing.TD01510.Charney.PlatesP1-P4/outgoing.TD01510.PlatesP3-P4/203865420006/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01510_Charney_GSA/outgoing.TD01510.Charney.PlatesP1-P4/outgoing.TD01510.PlatesP3-P4/203865420008/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01510_Charney_GSA/outgoing.TD01510.Charney.PlatesP1-P4/outgoing.TD01510.PlatesP3-P4/203865420010/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01510_Charney_GSA/outgoing.TD01510.Charney.PlatesP1-P4/outgoing.TD01510.PlatesP3-P4/203865420014/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01510_Charney_GSA/outgoing.TD01510.Charney.PlatesP1-P4/outgoing.TD01510.PlatesP3-P4/203865420017/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01510_Charney_GSA/outgoing.TD01510.Charney.PlatesP1-P4/outgoing.TD01510.PlatesP3-P4/203865420030/*idat gs://bigdeli-working/pakistan/idats

gsutil cp gs://bigdeli-working/pakistan/TD01601_Charney_GSA/outgoing.TD01601.Charney.PlatesP5-8/outgoing.TD01601.PlatesP5-P6/204391780006/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01601_Charney_GSA/outgoing.TD01601.Charney.PlatesP5-8/outgoing.TD01601.PlatesP5-P6/204425730001/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01601_Charney_GSA/outgoing.TD01601.Charney.PlatesP5-8/outgoing.TD01601.PlatesP5-P6/204425730010/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01601_Charney_GSA/outgoing.TD01601.Charney.PlatesP5-8/outgoing.TD01601.PlatesP5-P6/204425730033/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01601_Charney_GSA/outgoing.TD01601.Charney.PlatesP5-8/outgoing.TD01601.PlatesP5-P6/204425730044/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01601_Charney_GSA/outgoing.TD01601.Charney.PlatesP5-8/outgoing.TD01601.PlatesP5-P6/204425730045/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01601_Charney_GSA/outgoing.TD01601.Charney.PlatesP5-8/outgoing.TD01601.PlatesP5-P6/204425730140/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01601_Charney_GSA/outgoing.TD01601.Charney.PlatesP5-8/outgoing.TD01601.PlatesP5-P6/204425730141/*idat gs://bigdeli-working/pakistan/idats

gsutil cp gs://bigdeli-working/pakistan/TD01601_Charney_GSA/outgoing.TD01601.Charney.PlatesP5-8/outgoing.TD01601.PlatesP7-P8/204369630025/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01601_Charney_GSA/outgoing.TD01601.Charney.PlatesP5-8/outgoing.TD01601.PlatesP7-P8/204369630047/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01601_Charney_GSA/outgoing.TD01601.Charney.PlatesP5-8/outgoing.TD01601.PlatesP7-P8/204369630049/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01601_Charney_GSA/outgoing.TD01601.Charney.PlatesP5-8/outgoing.TD01601.PlatesP7-P8/204369630079/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01601_Charney_GSA/outgoing.TD01601.Charney.PlatesP5-8/outgoing.TD01601.PlatesP7-P8/204369630110/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01601_Charney_GSA/outgoing.TD01601.Charney.PlatesP5-8/outgoing.TD01601.PlatesP7-P8/204369630113/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01601_Charney_GSA/outgoing.TD01601.Charney.PlatesP5-8/outgoing.TD01601.PlatesP7-P8/204369630119/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01601_Charney_GSA/outgoing.TD01601.Charney.PlatesP5-8/outgoing.TD01601.PlatesP7-P8/204369630133/*idat gs://bigdeli-working/pakistan/idats

gsutil cp gs://bigdeli-working/pakistan/TD01793_Charney_GSA/outgoing.TD01793.Charney.P9-P12/outgoing.TD01793.PlatesP9-P10/204012240023/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01793_Charney_GSA/outgoing.TD01793.Charney.P9-P12/outgoing.TD01793.PlatesP9-P10/204012240024/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01793_Charney_GSA/outgoing.TD01793.Charney.P9-P12/outgoing.TD01793.PlatesP9-P10/204012240057/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01793_Charney_GSA/outgoing.TD01793.Charney.P9-P12/outgoing.TD01793.PlatesP9-P10/204012240062/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01793_Charney_GSA/outgoing.TD01793.Charney.P9-P12/outgoing.TD01793.PlatesP9-P10/204012240070/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01793_Charney_GSA/outgoing.TD01793.Charney.P9-P12/outgoing.TD01793.PlatesP9-P10/204012240071/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01793_Charney_GSA/outgoing.TD01793.Charney.P9-P12/outgoing.TD01793.PlatesP9-P10/204012240076/*idat gs://bigdeli-working/pakistan/idats

gsutil cp gs://bigdeli-working/pakistan/TD01793_Charney_GSA/outgoing.TD01793.Charney.P9-P12/outgoing.TD01793.PlatesP11-P12/203925040024/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01793_Charney_GSA/outgoing.TD01793.Charney.P9-P12/outgoing.TD01793.PlatesP11-P12/203925040041/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01793_Charney_GSA/outgoing.TD01793.Charney.P9-P12/outgoing.TD01793.PlatesP11-P12/203925040042/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01793_Charney_GSA/outgoing.TD01793.Charney.P9-P12/outgoing.TD01793.PlatesP11-P12/203925040059/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01793_Charney_GSA/outgoing.TD01793.Charney.P9-P12/outgoing.TD01793.PlatesP11-P12/203925040060/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01793_Charney_GSA/outgoing.TD01793.Charney.P9-P12/outgoing.TD01793.PlatesP11-P12/203925040066/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01793_Charney_GSA/outgoing.TD01793.Charney.P9-P12/outgoing.TD01793.PlatesP11-P12/203925040084/*idat gs://bigdeli-working/pakistan/idats


gsutil cp gs://bigdeli-working/pakistan/TD01828_Charney_GSA/outgoing.TD01828.Charney.P13-P16/outgoing.TD01828.PlatesP13-P14/204012240015/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01828_Charney_GSA/outgoing.TD01828.Charney.P13-P16/outgoing.TD01828.PlatesP13-P14/204012240029/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01828_Charney_GSA/outgoing.TD01828.Charney.P13-P16/outgoing.TD01828.PlatesP13-P14/204012240037/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01828_Charney_GSA/outgoing.TD01828.Charney.P13-P16/outgoing.TD01828.PlatesP13-P14/204012240038/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01828_Charney_GSA/outgoing.TD01828.Charney.P13-P16/outgoing.TD01828.PlatesP13-P14/204012240046/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01828_Charney_GSA/outgoing.TD01828.Charney.P13-P16/outgoing.TD01828.PlatesP13-P14/204012240056/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01828_Charney_GSA/outgoing.TD01828.Charney.P13-P16/outgoing.TD01828.PlatesP13-P14/204012240063/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01828_Charney_GSA/outgoing.TD01828.Charney.P13-P16/outgoing.TD01828.PlatesP13-P14/204012240078/*idat gs://bigdeli-working/pakistan/idats

gsutil cp gs://bigdeli-working/pakistan/TD01828_Charney_GSA/outgoing.TD01828.Charney.P13-P16/outgoing.TD01828.PlatesP15-P16/203865420007/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01828_Charney_GSA/outgoing.TD01828.Charney.P13-P16/outgoing.TD01828.PlatesP15-P16/203865420016/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01828_Charney_GSA/outgoing.TD01828.Charney.P13-P16/outgoing.TD01828.PlatesP15-P16/203865420019/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01828_Charney_GSA/outgoing.TD01828.Charney.P13-P16/outgoing.TD01828.PlatesP15-P16/203865420020/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01828_Charney_GSA/outgoing.TD01828.Charney.P13-P16/outgoing.TD01828.PlatesP15-P16/203865420029/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01828_Charney_GSA/outgoing.TD01828.Charney.P13-P16/outgoing.TD01828.PlatesP15-P16/203865420048/*idat gs://bigdeli-working/pakistan/idats

gsutil cp gs://bigdeli-working/pakistan/TD01876_Charney_GSA/outgoing.TD01876.Charney.PlatesP17-P20/outgoing.TD01876.PlatesP17-P18/203925040010/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01876_Charney_GSA/outgoing.TD01876.Charney.PlatesP17-P20/outgoing.TD01876.PlatesP17-P18/203925040014/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01876_Charney_GSA/outgoing.TD01876.Charney.PlatesP17-P20/outgoing.TD01876.PlatesP17-P18/203925040031/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01876_Charney_GSA/outgoing.TD01876.Charney.PlatesP17-P20/outgoing.TD01876.PlatesP17-P18/203925040039/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01876_Charney_GSA/outgoing.TD01876.Charney.PlatesP17-P20/outgoing.TD01876.PlatesP17-P18/203925040055/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01876_Charney_GSA/outgoing.TD01876.Charney.PlatesP17-P20/outgoing.TD01876.PlatesP17-P18/203925040070/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01876_Charney_GSA/outgoing.TD01876.Charney.PlatesP17-P20/outgoing.TD01876.PlatesP17-P18/203925040076/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01876_Charney_GSA/outgoing.TD01876.Charney.PlatesP17-P20/outgoing.TD01876.PlatesP17-P18/203925040078/*idat gs://bigdeli-working/pakistan/idats


gsutil cp gs://bigdeli-working/pakistan/TD01876_Charney_GSA/outgoing.TD01876.Charney.PlatesP17-P20/outgoing.TD01876.PlatesP19-P20/203865420037/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01876_Charney_GSA/outgoing.TD01876.Charney.PlatesP17-P20/outgoing.TD01876.PlatesP19-P20/203865420043/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01876_Charney_GSA/outgoing.TD01876.Charney.PlatesP17-P20/outgoing.TD01876.PlatesP19-P20/203925040021/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01876_Charney_GSA/outgoing.TD01876.Charney.PlatesP17-P20/outgoing.TD01876.PlatesP19-P20/203925040046/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01876_Charney_GSA/outgoing.TD01876.Charney.PlatesP17-P20/outgoing.TD01876.PlatesP19-P20/203925040049/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01876_Charney_GSA/outgoing.TD01876.Charney.PlatesP17-P20/outgoing.TD01876.PlatesP19-P20/203925040050/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01876_Charney_GSA/outgoing.TD01876.Charney.PlatesP17-P20/outgoing.TD01876.PlatesP19-P20/204311020004/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01876_Charney_GSA/outgoing.TD01876.Charney.PlatesP17-P20/outgoing.TD01876.PlatesP19-P20/204311020016/*idat gs://bigdeli-working/pakistan/idats

gsutil cp gs://bigdeli-working/pakistan/TD01896_Charney_GSA/outgoing.TD01896.Charney.PlatesP21-P24/outgoing.TD01896.PlatesP21-P22/203925040032/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01896_Charney_GSA/outgoing.TD01896.Charney.PlatesP21-P24/outgoing.TD01896.PlatesP21-P22/203925040056/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01896_Charney_GSA/outgoing.TD01896.Charney.PlatesP21-P24/outgoing.TD01896.PlatesP21-P22/203925040057/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01896_Charney_GSA/outgoing.TD01896.Charney.PlatesP21-P24/outgoing.TD01896.PlatesP21-P22/204368910013/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01896_Charney_GSA/outgoing.TD01896.Charney.PlatesP21-P24/outgoing.TD01896.PlatesP21-P22/204368910018/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01896_Charney_GSA/outgoing.TD01896.Charney.PlatesP21-P24/outgoing.TD01896.PlatesP21-P22/204368910027/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01896_Charney_GSA/outgoing.TD01896.Charney.PlatesP21-P24/outgoing.TD01896.PlatesP21-P22/204368910092/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01896_Charney_GSA/outgoing.TD01896.Charney.PlatesP21-P24/outgoing.TD01896.PlatesP21-P22/204368910095/*idat gs://bigdeli-working/pakistan/idats

gsutil cp gs://bigdeli-working/pakistan/TD01896_Charney_GSA/outgoing.TD01896.Charney.PlatesP21-P24/outgoing.TD01896.PlatesP23-P24/204369630039/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01896_Charney_GSA/outgoing.TD01896.Charney.PlatesP21-P24/outgoing.TD01896.PlatesP23-P24/204369630048/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01896_Charney_GSA/outgoing.TD01896.Charney.PlatesP21-P24/outgoing.TD01896.PlatesP23-P24/204369630050/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01896_Charney_GSA/outgoing.TD01896.Charney.PlatesP21-P24/outgoing.TD01896.PlatesP23-P24/204369630074/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01896_Charney_GSA/outgoing.TD01896.Charney.PlatesP21-P24/outgoing.TD01896.PlatesP23-P24/204369630093/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01896_Charney_GSA/outgoing.TD01896.Charney.PlatesP21-P24/outgoing.TD01896.PlatesP23-P24/204369630115/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01896_Charney_GSA/outgoing.TD01896.Charney.PlatesP21-P24/outgoing.TD01896.PlatesP23-P24/204369630129/*idat gs://bigdeli-working/pakistan/idats

gsutil cp gs://bigdeli-working/pakistan/TD01973_Charney_GSA/outgoing.TD01973.Charney.PlatesP25-P28/outgoing.TD01973.PlatesP25-P26/204369630037/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01973_Charney_GSA/outgoing.TD01973.Charney.PlatesP25-P28/outgoing.TD01973.PlatesP25-P26/204369630051/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01973_Charney_GSA/outgoing.TD01973.Charney.PlatesP25-P28/outgoing.TD01973.PlatesP25-P26/204369630076/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01973_Charney_GSA/outgoing.TD01973.Charney.PlatesP25-P28/outgoing.TD01973.PlatesP25-P26/204369630081/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01973_Charney_GSA/outgoing.TD01973.Charney.PlatesP25-P28/outgoing.TD01973.PlatesP25-P26/204369630099/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01973_Charney_GSA/outgoing.TD01973.Charney.PlatesP25-P28/outgoing.TD01973.PlatesP25-P26/204369630100/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01973_Charney_GSA/outgoing.TD01973.Charney.PlatesP25-P28/outgoing.TD01973.PlatesP25-P26/204369630111/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01973_Charney_GSA/outgoing.TD01973.Charney.PlatesP25-P28/outgoing.TD01973.PlatesP25-P26/204369630121/*idat gs://bigdeli-working/pakistan/idats

gsutil cp gs://bigdeli-working/pakistan/TD01973_Charney_GSA/outgoing.TD01973.Charney.PlatesP25-P28/outgoing.TD01973.PlatesP27-P28/204369620025/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01973_Charney_GSA/outgoing.TD01973.Charney.PlatesP25-P28/outgoing.TD01973.PlatesP27-P28/204369620026/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01973_Charney_GSA/outgoing.TD01973.Charney.PlatesP25-P28/outgoing.TD01973.PlatesP27-P28/204369620124/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01973_Charney_GSA/outgoing.TD01973.Charney.PlatesP25-P28/outgoing.TD01973.PlatesP27-P28/204369620125/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01973_Charney_GSA/outgoing.TD01973.Charney.PlatesP25-P28/outgoing.TD01973.PlatesP27-P28/204369620126/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01973_Charney_GSA/outgoing.TD01973.Charney.PlatesP25-P28/outgoing.TD01973.PlatesP27-P28/204369620130/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01973_Charney_GSA/outgoing.TD01973.Charney.PlatesP25-P28/outgoing.TD01973.PlatesP27-P28/204369620131/*idat gs://bigdeli-working/pakistan/idats
gsutil cp gs://bigdeli-working/pakistan/TD01973_Charney_GSA/outgoing.TD01973.Charney.PlatesP25-P28/outgoing.TD01973.PlatesP27-P28/204369620132/*idat gs://bigdeli-working/pakistan/idats


## testing to get the number of samples in the plate sheet across plates on local computer (only the sample plate information, no genotype data downloaded)
#format the ID files

awk '(NR>11){print $0}' pakistan_TD01510_Charney_GSA_outgoing.TD01510.Charney.PlatesP1-P4_outgoing.TD01510.PlatesP1-P2_Samplesheet.TD01510.PlatesP1-P2.csv > pakistan_gsa_plate_1_2
awk '(NR>11){print $0}' pakistan_TD01510_Charney_GSA_outgoing.TD01510.Charney.PlatesP1-P4_outgoing.TD01510.PlatesP3-P4_Samplesheet.TD01510.PlatesP3-P4.csv > pakistan_gsa_plate_3_4
awk '(NR>11){print $0}' pakistan_TD01601_Charney_GSA_outgoing.TD01601.Charney.PlatesP5-8_outgoing.TD01601.PlatesP5-P6_Samplesheet.TD01601.REDO.PlatesP5-P6.csv > pakistan_gsa_plate_5_6
awk '(NR>11){print $0}' pakistan_TD01601_Charney_GSA_outgoing.TD01601.Charney.PlatesP5-8_outgoing.TD01601.PlatesP7-P8_Samplesheet.TD01601.REDO.PlatesP7-P8.csv > pakistan_gsa_plate_7_8
awk '(NR>11){print $0}' pakistan_TD01793_Charney_GSA_outgoing.TD01793.Charney.P9-P12_outgoing.TD01793.PlatesP9-P10_Samplesheet.TD01793.PlatesP9-P10.csv > pakistan_gsa_plate_9_10
awk '(NR>11){print $0}' pakistan_TD01793_Charney_GSA_outgoing.TD01793.Charney.P9-P12_outgoing.TD01793.PlatesP11-P12_Samplesheet.TD01793.PlatesP11-P12.csv > pakistan_gsa_plate_11_12
awk '(NR>11){print $0}' pakistan_TD01828_Charney_GSA_outgoing.TD01828.Charney.P13-P16_outgoing.TD01828.PlatesP13-P14_Samplesheet.TD01828.PlatesP13-P14.csv > pakistan_gsa_plate_13_14
awk '(NR>11){print $0}' pakistan_TD01828_Charney_GSA_outgoing.TD01828.Charney.P13-P16_outgoing.TD01828.PlatesP15-P16_Samplesheet.TD01828.PlatesP15-P16.csv > pakistan_gsa_plate_15_16
awk '(NR>11){print $0}' pakistan_TD01876_Charney_GSA_outgoing.TD01876.Charney.PlatesP17-P20_outgoing.TD01876.PlatesP17-P18_Samplesheet.TD01876.PlatesP17-P18.csv > pakistan_gsa_plate_17_18
awk '(NR>11){print $0}' pakistan_TD01876_Charney_GSA_outgoing.TD01876.Charney.PlatesP17-P20_outgoing.TD01876.PlatesP19-P20_Samplesheet.TD01876.PlatesP19-P20.csv > pakistan_gsa_plate_19_20
awk '(NR>11){print $0}' pakistan_TD01896_Charney_GSA_outgoing.TD01896.Charney.PlatesP21-P24_outgoing.TD01896.PlatesP21-P22_Samplesheet.TD01896.PlatesP21-P22.csv > pakistan_gsa_plate_21_22
awk '(NR>11){print $0}' pakistan_TD01896_Charney_GSA_outgoing.TD01896.Charney.PlatesP21-P24_outgoing.TD01896.PlatesP23-P24_Samplesheet.TD01896.PlatesP23-P24.csv > pakistan_gsa_plate_23_24
awk '(NR>11){print $0}' pakistan_TD01973_Charney_GSA_outgoing.TD01973.Charney.PlatesP25-P28_outgoing.TD01973.PlatesP25-P26_Samplesheet.TD01973.PlatesP25-P26.csv > pakistan_gsa_plate_25_26
awk '(NR>11){print $0}' pakistan_TD01973_Charney_GSA_outgoing.TD01973.Charney.PlatesP25-P28_outgoing.TD01973.PlatesP27-P28_Samplesheet.TD01973.PlatesP27-P28.csv > pakistan_gsa_plate_27_28

#merge all the IDs together
cat pakistan_gsa_plate_1_2 pakistan_gsa_plate_3_4 pakistan_gsa_plate_5_6 pakistan_gsa_plate_7_8 pakistan_gsa_plate_9_10 pakistan_gsa_plate_11_12 pakistan_gsa_plate_13_14 pakistan_gsa_plate_15_16 pakistan_gsa_plate_17_18 pakistan_gsa_plate_19_20 pakistan_gsa_plate_21_22 pakistan_gsa_plate_23_24 pakistan_gsa_plate_25_26 pakistan_gsa_plate_27_28 > pakistan_gsa.sample 

#first replace space with _
sed 's/\ /\_/g' pakistan_gsa.sample > pakistan_gsa.sample.v2

#next replace , to a space
sed 's/\,/ /g' pakistan_gsa.sample.v2 > pakistan_gsa.sample.v3

#add column titles
#because you are on OSX , and you should have a newline after \ as POSIX specification and GNU sed allows that. like
sed -e '1i\'$'\n''Sample_ID SentrixBarcode_A SentrixPosition_A Sample_Plate Sample_Well' pakistan_gsa.sample.v3  >  pakistan_gsa.sample.header.csv

awk '{print $1}'  pakistan_gsa.sample.header.csv|uniq -c |wc -l
#2589 unique IDs 
awk '{print $2}'  pakistan_gsa.sample.header.csv|uniq -c |wc -l
#109 Sentrix barcodes

#print the line not starting with ID
grep -v '^8' pakistan_gsa.sample.header.csv 

#51 non-typical IDs
#1 empty line

#print the cell not starting with typical batch ID 
awk '{print $2}' pakistan_gsa.sample.header.csv | grep -v '^2'
#one line that is empty

#print the line starting with space
grep  "^ " pakistan_gsa.sample.header.csv |wc -l

#get rid of the empty line
grep -v "^ " pakistan_gsa.sample.header.csv  > pakistan_gsa.sample.final

awk '{print $2}'  pakistan_gsa.sample.final|uniq -c |sort|wc -l   
#108 batches (109 with header)
awk '{print $1}'  pakistan_gsa.sample.final|uniq -c |sort|wc -l   
#2588 unique IDs (2589 with header)

## testing on local computer ends ###


### Back to the Google Cloud

#after starting the jiayi-vm and click ssh
# copy the sample ID sheet from Google Bucket to Google VM
gsutil cp gs://bigdeli-working/pakistan/testing/pakistan_gsa_plate .

wc -l pakistan_gsa_plate
#2588 participants (1 empty line, 2588+1=2589)

grep -v "^," pakistan_gsa_plate  > test

mv test pakistan_gsa_plate
#2588 samples

# get the number of individuals with idat data (2568 out of 2588)
gsutil du gs://bigdeli-working/pakistan/idats > idat_grn_list
sed 's/\//\t/g' idat_grn_list |awk '{if ($1 ~ /^9/) print $6}' > idat_grn_list2

# $1 tells awk to look at the first "column"; ~ tells awk to do a RegularExpression match /..../ is a Regular expression; Within the RE is the string Linux and the special character ^;
# ^ causes the RE to match from the start (as opposed to matching anywhere in the line).

awk '{if ($1 ~ /Grn.idat$/) print $0}' idat_grn_list2 > idat_grn_idat_list 
awk '{if ($1 ~ /Red.idat$/) print $0}' idat_grn_list2 > idat_red_idat_list 
#2568 green idat and 2568 red idat  (a difference of 20 participants from 2588)


#First, We also found that 1 batch did not have idat data 
#203865420044 batch on plate 4 do not have idat data, PT-0102 to PT-0131, 24 participants in total, so leave with 2565 participants with idats, 2588-24=2564)!!! 

#Second, we found 4 samples with idat data not on the sample ID sheet
#thus added those back to the sample sheet manually (2564+4=2568, thus this matches with the number of idat files)

#the four ID below are not found in the sample sheet
204369630049_R06C02_Grn.idat
204369630049_R08C02_Grn.idat
204369630049_R10C02_Grn.idat
204369630049_R12C02_Grn.idat

#awk '{print $3}' pakistan_gsa.sample.tsv |sort > test_list
#sort idat_grn_idat_list > test_green_list
#comm -13 test_list test_green_list
##find lines only in file1
#comm -23 file1 file2 
#find lines only in file2
#comm -13 file1 file2 
#find lines common to both files
#comm -12 file1 file2 


#Third, we found the same barcode on different plates (plate 9 and 11)
#24 samples have the same idat file names (have to change the barcode so that the batch.tsv file wont get confused)

#rename the barcode for one of the two chips

#there are two Sentrix barcodes that are exactly the same
#change one of the barcode 203925040059 to a made-up code (204012240022) so that all the samples will be analyzed

#copy the samples with the 203925040059 barcode on plate 9 in Google Bucket to the Google VM
gsutil cp gs://bigdeli-working/pakistan/TD01793_Charney_GSA/outgoing.TD01793.Charney.P9-P12/outgoing.TD01793.PlatesP9-P10/203925040059/*idat .

# change the barcode name (in the file name, not in the file content) to the made-up code
find . -name '*idat' -exec bash -c ' mv $0 ${0/203925040059/204012240022}' {} \;

# copy the samples with the changed barcode back to the google bucket
gsutil cp *idat gs://bigdeli-working/pakistan/idats

#count the number of .idat files 
gsutil du gs://bigdeli-working/pakistan/idats| wc -l

#updated version: 2568 ppts


#To make the sample sheet (pakistan_gsa.sample.tsv) 
#with the pakistan_gsa_plate file includes the updated barcode ID (made-up for 1 Sentrix barcode for 24 samples), and excludea another 24 samples on the sheet that don't have idat information

#change the separator from , to tab ((MoChA can only read tab delimited)
sed 's/\,/\t/g'  pakistan_gsa_plate > pakistan_gsa_plate_v1

#change the barcode of 203925040059 on plate 9 to 204012240022 (make-up batch code!!!)
awk '{if ($2=="203925040059" && $4=="TD01793_Plate9") print $1,"204012240022",$3,$4,$5; else print $0}' pakistan_gsa_plate_v1 > pakistan_gsa_plate_v1.edit

#add 4 participant samples into the sample sheet (204369630049_R06C02_Grn.idat, 204369630049_R08C02_Grn.idat, 204369630049_R10C02_Grn.idat, 204369630049_R12C02_Grn.idat)
cat 4ppt_add pakistan_gsa_plate_v1.edit > pakistan_gsa_plate_v1.edit2

awk '{print $2"_"$3"_Grn.idat", $7= $2"_"$3"_Red.idat"}'  pakistan_gsa_plate_v1.edit2 >   pakistan_gsa_plate_v2
awk '{print $1,$2}' pakistan_gsa_plate_v1.edit2 > pakistan_gsa_plate_v3 
paste pakistan_gsa_plate_v3  pakistan_gsa_plate_v2 > pakistan_gsa_plate_v4
sed '1i sample_id batch_id green_idat red_idat' pakistan_gsa_plate_v4 > pakistan_gsa_plate_v5

#remove the 24 samples without idat information (203865420044 batch on plate 4)

awk '{if ($2!="203865420044" ) print $0}' pakistan_gsa_plate_v5 > pakistan_gsa_plate_v6

#change all batch ID to 1 (based on Giulio's suggestion)
# based on Giulio's suggestion:  that is not advisable as it would create very small batches. Also, for running MoChA, 
#some adjustments would only happen when running many samples together. I usually recommend a few thousand samples per batch if possible. I would run as one batch.

awk '{ if(NR==1) print $0; else if (NR>1) print $1,"1",$3,$4}' pakistan_gsa_plate_v6  > pakistan_gsa_plate_v7

#change the separator from space to tab again (MoChA can only read tab delimited)
sed 's/ /\t/g' pakistan_gsa_plate_v7 > pakistan_gsa.sample.tsv


# sample sheet is created! 



## And next for the batch sheet (pakistan_gsa.batch.tsv)!

#keep the uniq batch ID
awk '(NR>1){print $2,$3="GSA-24v3-0_A2.csv",$4="GSA-24v3-0_A2.bpm",$5="GSA-24v3-0_A1_ClusterFile.egt"}' pakistan_gsa.sample.tsv |sort -k1| uniq |sed '1i batch_id csv bpm egt' > test.batch.tsv

#change the separator from space to tab again (MoChA can only read tab delimited)
sed 's/ /\t/g' test.batch.tsv > pakistan_gsa.batch.tsv


# copy the sample sheet and batch sheet from VM to Google bucket for MoChA pipeline to read later 
gsutil cp pakistan_gsa.batch.tsv gs://bigdeli-working/pakistan/samples
gsutil cp pakistan_gsa.sample.tsv gs://bigdeli-working/pakistan/samples


#updated: 2569 rows for sample sheet (including header), 2 rows for batch sheet (including header)

# rename the Google bucket folder to GRCh38
gsutil mv gs://bigdeli-working/pakistan/mocha.GRCh38 gs://bigdeli-working/pakistan/GRCh38
